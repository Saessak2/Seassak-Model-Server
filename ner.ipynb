{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.3.attention.self.value.weight', 'discriminator_predictions.dense.bias', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.11.output.LayerNorm.bias', 'electra.embeddings_project.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.embeddings_project.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.output.LayerNorm.weight', 'discriminator_predictions.dense_prediction.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.0.output.dense.weight', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.embeddings.LayerNorm.weight', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'discriminator_predictions.dense_prediction.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'discriminator_predictions.dense.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.2.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForTokenClassification\n",
    "\n",
    "model = TFBertForTokenClassification.from_pretrained(\"monologg/kocharelectra-small-discriminator\", num_labels=9, from_pt=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.hf_compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 샘플 개수 : 13344\n",
      "테스트 데이터 샘플 개수 : 3336\n",
      "개체명 태깅 정보 : ['O', 'PLT-B', 'PLT-I', 'BUG-B', 'BUG-I', 'DIS-B', 'DIS-I', 'CTG-B', 'CTG-I']\n",
      "개체명 태깅 정보의 개수 :  9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datapath ='/Users/eunbin/Desktop/Projects/saessack-server/extra/data/kinnaver/sent_tag.json'\n",
    "data = open(datapath, encoding='utf-8')\n",
    "df = pd.DataFrame(json.load(data))\n",
    "# df.head()\n",
    "train_ner_df = df.iloc[:13344, 0:]\n",
    "test_ner_df = df.iloc[13344:, 0:]\n",
    "print(\"학습 데이터 샘플 개수 :\", len(train_ner_df))\n",
    "print(\"테스트 데이터 샘플 개수 :\", len(test_ner_df))\n",
    "\n",
    "train_data_sentence = [sent.split() for sent in train_ner_df['sentence'].values]\n",
    "test_data_sentence = [sent.split() for sent in test_ner_df['sentence'].values]\n",
    "train_data_label = [tag.split() for tag in train_ner_df['tagging'].values]\n",
    "test_data_label = [tag.split() for tag in test_ner_df['tagging'].values]\n",
    "# print(train_data_sentence[0])\n",
    "# print(train_data_label[0])\n",
    "\n",
    "ner_label_path = '/Users/eunbin/Desktop/Projects/saessack-server/extra/data/tag/ner_label.txt'\n",
    "labels = [label.strip() for label in open(ner_label_path, 'r', encoding='utf-8')]\n",
    "print('개체명 태깅 정보 :', labels)\n",
    "\n",
    "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
    "index_to_tag = {index: tag for index, tag in enumerate(labels)}\n",
    "tag_size = len(tag_to_index)\n",
    "print(\"개체명 태깅 정보의 개수 : \", tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'KoCharElectraTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from tokenization_kocharelectra import KoCharElectraTokenizer\n",
    "\n",
    "tokenizer = KoCharElectraTokenizer.from_pretrained(\"monologg/kocharelectra-base-discriminator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method KoCharElectraTokenizer.get_vocab of PreTrainedTokenizer(name_or_path='monologg/kocharelectra-base-discriminator', vocab_size=11568, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer,\n",
    "                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
    "    cls_token = '[CLS]'\n",
    "    sep_token = '[SEP]'\n",
    "    pad_token_id = 0\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "\n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        tokens = []\n",
    "        labels_ids = []\n",
    "        for one_word, label_token in zip(example, label):\n",
    "            tokens.append(one_word)\n",
    "            labels_ids.append(tag_to_index[label_token])\n",
    "\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
    "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        labels_ids += [pad_token_id_for_label]\n",
    "        tokens = [cls_token] + tokens\n",
    "        labels_ids = [pad_token_id_for_label] + labels_ids\n",
    "\n",
    "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        padding_count = max_seq_len - len(input_id)\n",
    "\n",
    "        input_id = input_id + ([pad_token_id] * padding_count)\n",
    "        attention_mask = attention_mask + ([0] * padding_count)\n",
    "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
    "        label = labels_ids + ([pad_token_id_for_label] * padding_count)\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "        assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13344/13344 [00:02<00:00, 4543.65it/s]\n",
      "100%|██████████| 3336/3336 [00:00<00:00, 4751.54it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_examples_to_features(train_data_sentence, train_data_label, max_seq_len=512, tokenizer=tokenizer)\n",
    "X_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label, max_seq_len=512, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1score(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def sequences_to_tags(self, label_ids, pred_ids):\n",
    "      label_list = []\n",
    "      pred_list = []\n",
    "\n",
    "      for i in range(0, len(label_ids)):\n",
    "        label_tag = []\n",
    "        pred_tag = []\n",
    "\n",
    "        for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
    "          if label_index != -100:\n",
    "            label_tag.append(index_to_tag[label_index])\n",
    "            pred_tag.append(index_to_tag[pred_index])\n",
    "        \n",
    "        label_list.append(label_tag)\n",
    "        pred_list.append(pred_tag)\n",
    "\n",
    "      return label_list, pred_list\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "      y_predicted = self.model.predict(self.X_test)\n",
    "      y_predicted = np.argmax(y_predicted.logits, axis = 2)\n",
    "\n",
    "      label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
    "\n",
    "      score = f1_score(label_list, pred_list, suffix=True)\n",
    "      print(' - f1: {:04.2f}'.format(score * 100))\n",
    "      print(classification_report(label_list, pred_list, suffix=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "f1_score_report = F1score(X_test, y_test)\n",
    "\n",
    "model.compile\n",
    "model.fit(\n",
    "    X_train, y_train, epochs=7, batch_size=32,\n",
    "    callbacks = [f1_score_report]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('python3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57c8f761e75d7748b11c72468e681dd62809eb3b233f69a9ae36853aa8b009bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
