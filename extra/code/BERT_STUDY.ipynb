{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aecc0a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74496864]\n",
      " [0.81688289]\n",
      " [0.76223718]\n",
      " [0.79366008]\n",
      " [0.73619291]\n",
      " [0.73978974]\n",
      " [0.71965782]\n",
      " [0.73208794]\n",
      " [0.76348267]\n",
      " [0.80329121]]\n",
      "[(0, 0.74496864), (1, 0.81688289), (2, 0.76223718), (3, 0.79366008), (4, 0.73619291), (5, 0.73978974), (6, 0.71965782), (7, 0.73208794), (8, 0.76348267), (9, 0.80329121)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "matrix = [[0.74496864],[0.81688289],[0.76223718],[0.79366008],[0.73619291], [0.73978974], [0.71965782], [0.73208794], [0.76348267], [0.80329121]]\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "print(matrix)\n",
    "\n",
    "sim = []\n",
    "i = 0\n",
    "for x in matrix:\n",
    "    sim.append((i, x[0]))\n",
    "    i+=1\n",
    "    \n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9206c71",
   "metadata": {},
   "source": [
    "## Bert Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4b6b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0738b65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deb03fa143443f88cc064b9a9a11dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForMaskedLM: ['bert.embeddings.position_ids', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing TFBertForMaskedLM from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMaskedLM from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175b79639f014581a34745ae01478bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b7d5bf5fae4908957b629d5b88d7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9217e72a33643a8a8caf74d0783e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa83438d9ef4f19aca79e7e098de596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from_pt는 해당 모델이 기존에는 텐서플로우가 아닌 파이토치로 학습된 모델이었지만 이를 텐서플로우에서 사용하겠단 의미.\n",
    "\n",
    "model = TFBertForMaskedLM.from_pretrained('klue/bert-base', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbceac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('축구는 정말 재미있는 [MASK]다.', return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e36b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[   2 4713 2259 3944 6001 2259    4  809   18    3]], shape=(1, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids']) # 정수인코딩 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdb003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0 0 0 0 0 0 0 0 0 0]], shape=(1, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['token_type_ids']) # 세그먼트 인코딩 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9536d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1 1 1 1 1 1 1 1 1 1]], shape=(1, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['attention_mask']) # 어텐션 마스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4632852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685574f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11891059577465057,\n",
       "  'token': 4200,\n",
       "  'token_str': '아들',\n",
       "  'sequence': '잘생긴 현민이는 이쁜 은빈이의 아들 이다.'},\n",
       " {'score': 0.11258185654878616,\n",
       "  'token': 5565,\n",
       "  'token_str': '동생',\n",
       "  'sequence': '잘생긴 현민이는 이쁜 은빈이의 동생 이다.'},\n",
       " {'score': 0.0790463387966156,\n",
       "  'token': 900,\n",
       "  'token_str': '딸',\n",
       "  'sequence': '잘생긴 현민이는 이쁜 은빈이의 딸 이다.'},\n",
       " {'score': 0.048177462071180344,\n",
       "  'token': 3781,\n",
       "  'token_str': '모습',\n",
       "  'sequence': '잘생긴 현민이는 이쁜 은빈이의 모습 이다.'},\n",
       " {'score': 0.04052730277180672,\n",
       "  'token': 4122,\n",
       "  'token_str': '엄마',\n",
       "  'sequence': '잘생긴 현민이는 이쁜 은빈이의 엄마 이다.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('잘생긴 현민이는 이쁜 은빈이의 [MASK]이다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07cef4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2562898099422455,\n",
       "  'token': 2265,\n",
       "  'token_str': 'show',\n",
       "  'sequence': 'the avengers is a really fun show.'},\n",
       " {'score': 0.17284077405929565,\n",
       "  'token': 3185,\n",
       "  'token_str': 'movie',\n",
       "  'sequence': 'the avengers is a really fun movie.'},\n",
       " {'score': 0.1110772043466568,\n",
       "  'token': 2466,\n",
       "  'token_str': 'story',\n",
       "  'sequence': 'the avengers is a really fun story.'},\n",
       " {'score': 0.07248972356319427,\n",
       "  'token': 2186,\n",
       "  'token_str': 'series',\n",
       "  'sequence': 'the avengers is a really fun series.'},\n",
       " {'score': 0.07046632468700409,\n",
       "  'token': 2143,\n",
       "  'token_str': 'film',\n",
       "  'sequence': 'the avengers is a really fun film.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('The Avengers is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538dab4",
   "metadata": {},
   "source": [
    "## Bert 다음 문장 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b685e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertForNextSentencePrediction\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e307357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForNextSentencePrediction: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForNextSentencePrediction from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForNextSentencePrediction from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForNextSentencePrediction were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForNextSentencePrediction for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForNextSentencePrediction.from_pretrained('klue/bert-base', from_pt=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe4464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 44), dtype=int32, numpy=\n",
       "array([[    2,  5791,  2440,  5339,  4713,  2104,  2124,  2259,  3708,\n",
       "         2145,  3965,  6233,  4414,  2496,  2359,  2414,  3665, 31221,\n",
       "         1751,  9900, 12190,    18,     3,  4062,  2069,   543,  2178,\n",
       "         2209,  3629,  2079,  5791,  2440,  5339,  4713,  2104,  7288,\n",
       "         3864,  2259,  5537,  2371,  2219,  3606,    18,     3]],\n",
       "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 44), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 44), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "      dtype=int32)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이어지는 두 개의 문장\n",
    "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
    "next_sentence = \"여행을 가보니 한국의 2002년 월드컵 축구대회의 준비는 완벽했습니다.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9da312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : [0]\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 함수를 지나기 전의 값인 logits을 리턴\n",
    "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
    "\n",
    "softmax = tf.keras.layers.Softmax()\n",
    "probs = softmax(logits)\n",
    "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())\n",
    "# 이어지면 0, 안이어지면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf2db0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 예측 레이블 : [1]\n"
     ]
    }
   ],
   "source": [
    "# 상관없는 두 개의 문장\n",
    "prompt = \"2002년 월드컵 축구대회는 일본과 공동으로 개최되었던 세계적인 큰 잔치입니다.\"\n",
    "next_sentence = \"극장가서 로맨스 영화를 보고싶어요\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
    "\n",
    "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
    "\n",
    "softmax = tf.keras.layers.Softmax()\n",
    "probs = softmax(logits)\n",
    "print('최종 예측 레이블 :', tf.math.argmax(probs, axis=-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08600cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f60e1cfc59bbbd32a22e9241c17b143103a1817ab9f9fb1cc9a27479f8daa683"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
